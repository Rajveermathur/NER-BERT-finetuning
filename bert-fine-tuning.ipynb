{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch>=2.0 (from -r requirements.txt (line 1))\n",
      "  Using cached torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers>=4.36 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.7 kB 165.2 kB/s eta 0:00:01\n",
      "     -------------------------------------  41.0/41.7 kB 219.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.7/41.7 kB 201.7 kB/s eta 0:00:00\n",
      "Collecting datasets>=2.17 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.20.1 (from -r requirements.txt (line 4))\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting seqeval>=1.2.2 (from -r requirements.txt (line 5))\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB 1.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting scikit-learn>=1.3.0 (from -r requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas>=2.0 (from -r requirements.txt (line 7))\n",
      "  Downloading pandas-2.3.1-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting filelock (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from torch>=2.0->-r requirements.txt (line 1)) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.9/60.9 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from transformers>=4.36->-r requirements.txt (line 2)) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch>=2.0->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from accelerate>=0.20.1->-r requirements.txt (line 4)) (7.0.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 6))\n",
      "  Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 6))\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 6))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from pandas>=2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.0->-r requirements.txt (line 7))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0->-r requirements.txt (line 7))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->-r requirements.txt (line 7)) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers>=4.36->-r requirements.txt (line 2))\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajveer mathur\\desktop\\ner-bert-finetuning\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers>=4.36->-r requirements.txt (line 2)) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0->-r requirements.txt (line 1))\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading multidict-6.6.3-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.17->-r requirements.txt (line 3))\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "     ---------------------------------------- 0.0/76.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.3/76.3 kB 4.4 MB/s eta 0:00:00\n",
      "Using cached torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.1/11.2 MB 36.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.2/11.2 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.2 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/11.2 MB 26.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.9/11.2 MB 26.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.2 MB 25.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.0/11.2 MB 25.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.2 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.8/11.2 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.2 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "   ---------------------------------------- 0.0/494.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 494.8/494.8 kB 15.6 MB/s eta 0:00:00\n",
      "Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "   ---------------------------------------- 0.0/367.1 kB ? eta -:--:--\n",
      "   --------------------------------------  358.4/367.1 kB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 367.1/367.1 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.7/8.9 MB 29.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.6/8.9 MB 25.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.4/8.9 MB 23.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 22.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/8.9 MB 23.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.7/8.9 MB 23.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 24.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 24.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.2/11.3 MB 25.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.6/11.3 MB 28.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.3 MB 27.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 26.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.1/11.3 MB 27.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 23.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.3 MB 21.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.3 MB 21.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.3 MB 21.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.3 MB 22.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.3 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "   ---------------------------------------- 0.0/193.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 193.6/193.6 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "   ---------------------------------------- 0.0/558.8 kB ? eta -:--:--\n",
      "   --------------------------------------  553.0/558.8 kB 33.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 558.8/558.8 kB 11.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 307.7/307.7 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.1/13.1 MB 23.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/13.1 MB 22.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.9/13.1 MB 20.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.9/13.1 MB 20.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.8/13.1 MB 20.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.6/13.1 MB 20.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.4/13.1 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/13.1 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.4/13.1 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/13.1 MB 19.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.3/13.1 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.3/13.1 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.1 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/26.2 MB 32.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 2.4/26.2 MB 25.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.6/26.2 MB 26.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.9/26.2 MB 26.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.2/26.2 MB 26.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.5/26.2 MB 26.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.1/26.2 MB 26.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.0/26.2 MB 25.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 11.2/26.2 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.3/26.2 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.3/26.2 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.9/26.2 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.8/26.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.5/26.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 16.3/26.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 17.0/26.2 MB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.8/26.2 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.3/26.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.2/26.2 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.3/26.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.6/26.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.8/26.2 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 16.0 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 509.2/509.2 kB 10.6 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 276.0/276.0 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.8/64.8 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/38.6 MB 42.9 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 2.6/38.6 MB 33.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 3.9/38.6 MB 30.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 29.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.5/38.6 MB 29.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 7.8/38.6 MB 29.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.1/38.6 MB 29.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.4/38.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 11.6/38.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 12.8/38.6 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 14.1/38.6 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 15.4/38.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 17.4/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 18.6/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 19.5/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.4/38.6 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 21.8/38.6 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 23.1/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.3/38.6 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.6/38.6 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 27.0/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.2/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.6/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.5/38.6 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.3/38.6 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.1/38.6 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.3/38.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.2/38.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.6/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 15.2 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.2/2.5 MB 37.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 30.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 20.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
      "   ---------------------------------------- 0.0/453.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 453.3/453.3 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 162.7/162.7 kB 10.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.4/105.4 kB 3.0 MB/s eta 0:00:00\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 129.8/129.8 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.6.3-cp311-cp311-win_amd64.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.9/45.9 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.7/86.7 kB 2.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml): started\n",
      "  Building wheel for seqeval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16283 sha256=c3325a89d395ac115e015d019f1500950b75e26930e3a687d361c66eea7bf2f0\n",
      "  Stored in directory: c:\\users\\rajveer mathur\\appdata\\local\\pip\\cache\\wheels\\bc\\92\\f0\\243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, pyarrow, propcache, numpy, networkx, multidict, MarkupSafe, joblib, idna, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, pandas, multiprocess, jinja2, aiosignal, torch, scikit-learn, huggingface-hub, aiohttp, tokenizers, seqeval, accelerate, transformers, datasets\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.7.14 charset_normalizer-3.4.2 datasets-4.0.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.34.3 idna-3.10 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 multidict-6.6.3 multiprocess-0.70.16 networkx-3.5 numpy-2.3.2 pandas-2.3.1 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.16.1 seqeval-1.2.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.7.1 tqdm-4.67.1 transformers-4.54.1 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Template</th>\n",
       "      <th>Filled Template</th>\n",
       "      <th>Tokenised Filled Template</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In our video conference, discuss the role of e...</td>\n",
       "      <td>In our video conference, discuss the role of e...</td>\n",
       "      <td>['in', 'our', 'video', 'conference', ',', 'dis...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you draft a letter for [NAME_1] to send ...</td>\n",
       "      <td>Could you draft a letter for Dietrich, Schulis...</td>\n",
       "      <td>['could', 'you', 'draft', 'a', 'letter', 'for'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discuss the options for [FULLNAME_1] who wants...</td>\n",
       "      <td>Discuss the options for Jeffery Pfeffer who wa...</td>\n",
       "      <td>['discuss', 'the', 'options', 'for', 'jeff', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B-FULLNAME', 'I-FULLNAME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13. Write a press release announcing [FULLNAME...</td>\n",
       "      <td>13. Write a press release announcing Gayle Wat...</td>\n",
       "      <td>['13', '.', 'write', 'a', 'press', 'release', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FULLNAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9. Develop an inventory management plan for [F...</td>\n",
       "      <td>9. Develop an inventory management plan for Ev...</td>\n",
       "      <td>['9', '.', 'develop', 'an', 'inventory', 'mana...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Template  \\\n",
       "0  In our video conference, discuss the role of e...   \n",
       "1  Could you draft a letter for [NAME_1] to send ...   \n",
       "2  Discuss the options for [FULLNAME_1] who wants...   \n",
       "3  13. Write a press release announcing [FULLNAME...   \n",
       "4  9. Develop an inventory management plan for [F...   \n",
       "\n",
       "                                     Filled Template  \\\n",
       "0  In our video conference, discuss the role of e...   \n",
       "1  Could you draft a letter for Dietrich, Schulis...   \n",
       "2  Discuss the options for Jeffery Pfeffer who wa...   \n",
       "3  13. Write a press release announcing Gayle Wat...   \n",
       "4  9. Develop an inventory management plan for Ev...   \n",
       "\n",
       "                           Tokenised Filled Template  \\\n",
       "0  ['in', 'our', 'video', 'conference', ',', 'dis...   \n",
       "1  ['could', 'you', 'draft', 'a', 'letter', 'for'...   \n",
       "2  ['discuss', 'the', 'options', 'for', 'jeff', '...   \n",
       "3  ['13', '.', 'write', 'a', 'press', 'release', ...   \n",
       "4  ['9', '.', 'develop', 'an', 'inventory', 'mana...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NA...  \n",
       "2  ['O', 'O', 'O', 'O', 'B-FULLNAME', 'I-FULLNAME...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FULLNAM...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FU...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Template</th>\n",
       "      <th>Filled Template</th>\n",
       "      <th>Tokenised Filled Template</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In our video conference, discuss the role of e...</td>\n",
       "      <td>In our video conference, discuss the role of e...</td>\n",
       "      <td>[in, our, video, conference, ,, discuss, the, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you draft a letter for [NAME_1] to send ...</td>\n",
       "      <td>Could you draft a letter for Dietrich, Schulis...</td>\n",
       "      <td>[could, you, draft, a, letter, for, dietrich, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-NAME, I-NAME, I-NAME, I-N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discuss the options for [FULLNAME_1] who wants...</td>\n",
       "      <td>Discuss the options for Jeffery Pfeffer who wa...</td>\n",
       "      <td>[discuss, the, options, for, jeff, ##ery, p, #...</td>\n",
       "      <td>[O, O, O, O, B-FULLNAME, I-FULLNAME, I-FULLNAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13. Write a press release announcing [FULLNAME...</td>\n",
       "      <td>13. Write a press release announcing Gayle Wat...</td>\n",
       "      <td>[13, ., write, a, press, release, announcing, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-FULLNAME, I-FULLNAME, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9. Develop an inventory management plan for [F...</td>\n",
       "      <td>9. Develop an inventory management plan for Ev...</td>\n",
       "      <td>[9, ., develop, an, inventory, management, pla...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-FULLNAME, I-FULLNAM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Template  \\\n",
       "0  In our video conference, discuss the role of e...   \n",
       "1  Could you draft a letter for [NAME_1] to send ...   \n",
       "2  Discuss the options for [FULLNAME_1] who wants...   \n",
       "3  13. Write a press release announcing [FULLNAME...   \n",
       "4  9. Develop an inventory management plan for [F...   \n",
       "\n",
       "                                     Filled Template  \\\n",
       "0  In our video conference, discuss the role of e...   \n",
       "1  Could you draft a letter for Dietrich, Schulis...   \n",
       "2  Discuss the options for Jeffery Pfeffer who wa...   \n",
       "3  13. Write a press release announcing Gayle Wat...   \n",
       "4  9. Develop an inventory management plan for Ev...   \n",
       "\n",
       "                           Tokenised Filled Template  \\\n",
       "0  [in, our, video, conference, ,, discuss, the, ...   \n",
       "1  [could, you, draft, a, letter, for, dietrich, ...   \n",
       "2  [discuss, the, options, for, jeff, ##ery, p, #...   \n",
       "3  [13, ., write, a, press, release, announcing, ...   \n",
       "4  [9, ., develop, an, inventory, management, pla...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, B-NAME, I-NAME, I-NAME, I-N...  \n",
       "2  [O, O, O, O, B-FULLNAME, I-FULLNAME, I-FULLNAM...  \n",
       "3  [O, O, O, O, O, O, O, B-FULLNAME, I-FULLNAME, ...  \n",
       "4  [O, O, O, O, O, O, O, O, B-FULLNAME, I-FULLNAM...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def try_parse_list(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return ast.literal_eval(cell)\n",
    "    return cell\n",
    "\n",
    "df[\"Tokenised Filled Template\"] = df[\"Tokenised Filled Template\"].apply(try_parse_list)\n",
    "df[\"Tokens\"] = df[\"Tokens\"].apply(try_parse_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at index 701\n",
      "Mismatch at index 1123\n",
      "Mismatch at index 1906\n",
      "Mismatch at index 2141\n",
      "Mismatch at index 2192\n",
      "Mismatch at index 2205\n",
      "Mismatch at index 2375\n",
      "Mismatch at index 2387\n",
      "Mismatch at index 3218\n",
      "Mismatch at index 3381\n",
      "Mismatch at index 3654\n",
      "Mismatch at index 3870\n",
      "Mismatch at index 4265\n",
      "Mismatch at index 4993\n",
      "Mismatch at index 5071\n",
      "Mismatch at index 5135\n",
      "Mismatch at index 6312\n",
      "Mismatch at index 6801\n",
      "Mismatch at index 7190\n",
      "Mismatch at index 7384\n",
      "Mismatch at index 7779\n",
      "Mismatch at index 7843\n",
      "Mismatch at index 8652\n",
      "Mismatch at index 8710\n",
      "Mismatch at index 10344\n",
      "Mismatch at index 10800\n",
      "Mismatch at index 11358\n",
      "Mismatch at index 12127\n",
      "Mismatch at index 12358\n",
      "Mismatch at index 13103\n",
      "Mismatch at index 13467\n",
      "Mismatch at index 13574\n",
      "Mismatch at index 13642\n",
      "Mismatch at index 13709\n",
      "Mismatch at index 14074\n",
      "Mismatch at index 14272\n",
      "Mismatch at index 14604\n",
      "Mismatch at index 15549\n",
      "Mismatch at index 15775\n",
      "Mismatch at index 15847\n",
      "Mismatch at index 16239\n",
      "Mismatch at index 16297\n",
      "Mismatch at index 16604\n",
      "Mismatch at index 16872\n",
      "Mismatch at index 17013\n",
      "Mismatch at index 17252\n",
      "Mismatch at index 18583\n",
      "Mismatch at index 18718\n",
      "Mismatch at index 18959\n",
      "Mismatch at index 19461\n",
      "Mismatch at index 19527\n",
      "Mismatch at index 19644\n",
      "Mismatch at index 19842\n",
      "Mismatch at index 19939\n",
      "Mismatch at index 19975\n",
      "Mismatch at index 20893\n",
      "Mismatch at index 21988\n",
      "Mismatch at index 22152\n",
      "Mismatch at index 22562\n",
      "Mismatch at index 22606\n",
      "Mismatch at index 22670\n",
      "Total mismatches found: 61\n"
     ]
    }
   ],
   "source": [
    "# Check for mismatches between tokens and labels\n",
    "counter = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if len(row[\"Tokenised Filled Template\"]) != len(row[\"Tokens\"]):\n",
    "        print(f\"Mismatch at index {idx}\")\n",
    "        # print(f\"Tokens ({len(row['Tokenised Filled Template'])}): {row['Tokenised Filled Template']}\")\n",
    "        # print(f\"Labels ({len(row['Tokens'])}): {row['Tokens']}\")\n",
    "        counter += 1\n",
    "print(f\"Total mismatches found: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter only rows where tokens and labels match in length\n",
    "df = df[df[\"Tokenised Filled Template\"].str.len() == df[\"Tokens\"].str.len()]\n",
    "# Check for mismatches between tokens and labels\n",
    "counter = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if len(row[\"Tokenised Filled Template\"]) != len(row[\"Tokens\"]):\n",
    "        print(f\"Mismatch at index {idx}\")\n",
    "        # print(f\"Tokens ({len(row['Tokenised Filled Template'])}): {row['Tokenised Filled Template']}\")\n",
    "        # print(f\"Labels ({len(row['Tokens'])}): {row['Tokens']}\")\n",
    "        counter += 1\n",
    "print(f\"Total mismatches found: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BIO tag scheme\n",
    "all_tags = set(tag for tags in df[\"Tokens\"] for tag in tags)\n",
    "unique_tags = sorted(all_tags)\n",
    "label2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2label = {idx: tag for tag, idx in label2id.items()}\n",
    "\n",
    "# Add numeric label ids for each row\n",
    "df[\"Label_ids\"] = df[\"Tokens\"].apply(lambda tags: [label2id[tag] for tag in tags])\n",
    "\n",
    "# Train-validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "def to_hf_dataset(dataframe):\n",
    "    return Dataset.from_dict({\n",
    "        \"tokens\": dataframe[\"Tokenised Filled Template\"].tolist(),\n",
    "        \"labels\": dataframe[\"Label_ids\"].tolist()\n",
    "    })\n",
    "\n",
    "train_dataset = to_hf_dataset(train_df)\n",
    "val_dataset = to_hf_dataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajveer.Mathur\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdbb8e03a234588b2c6fbcafeeae534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d94696dc0449e3ae1aea83d2e653c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2294 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        prev_word_id = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != prev_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Apply to datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFPreTrainedModel' from 'transformers' (C:\\Users\\Rajveer.Mathur\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2276\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2276\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2277\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2278\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2306\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2304\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2276\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2276\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2277\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2278\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2306\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2304\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\integrations\\integration_utils.py:42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_MODE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Running in WANDB offline mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel, TrainingArguments\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     PushToHubMixin,\n\u001b[0;32m     46\u001b[0m     flatten_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     logging,\n\u001b[0;32m     52\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TFPreTrainedModel' from 'transformers' (C:\\Users\\Rajveer.Mathur\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(predictions_output):\n",
    "    preds, labels = predictions_output\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        pred_tags = []\n",
    "        label_tags = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l != -100:\n",
    "                pred_tags.append(id2label[p])\n",
    "                label_tags.append(id2label[l])\n",
    "        true_preds.append(pred_tags)\n",
    "        true_labels.append(label_tags)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./final_ner_model\")\n",
    "tokenizer.save_pretrained(\"./final_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./final_ner_model\", tokenizer=\"./final_ner_model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Dr. Marvin Rolfson and Julius Daugherty attended the arbitration.\"\n",
    "print(ner_pipeline(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
