{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_list(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return ast.literal_eval(cell)\n",
    "    return cell\n",
    "\n",
    "df[\"Tokenised Filled Template\"] = df[\"Tokenised Filled Template\"].apply(try_parse_list)\n",
    "df[\"Tokens\"] = df[\"Tokens\"].apply(try_parse_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mismatches between tokens and labels\n",
    "counter = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if len(row[\"Tokenised Filled Template\"]) != len(row[\"Tokens\"]):\n",
    "        print(f\"Mismatch at index {idx}\")\n",
    "        # print(f\"Tokens ({len(row['Tokenised Filled Template'])}): {row['Tokenised Filled Template']}\")\n",
    "        # print(f\"Labels ({len(row['Tokens'])}): {row['Tokens']}\")\n",
    "        counter += 1\n",
    "print(f\"Total mismatches found: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only rows where tokens and labels match in length\n",
    "df = df[df[\"Tokenised Filled Template\"].str.len() == df[\"Tokens\"].str.len()]\n",
    "# Check for mismatches between tokens and labels\n",
    "counter = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if len(row[\"Tokenised Filled Template\"]) != len(row[\"Tokens\"]):\n",
    "        print(f\"Mismatch at index {idx}\")\n",
    "        # print(f\"Tokens ({len(row['Tokenised Filled Template'])}): {row['Tokenised Filled Template']}\")\n",
    "        # print(f\"Labels ({len(row['Tokens'])}): {row['Tokens']}\")\n",
    "        counter += 1\n",
    "print(f\"Total mismatches found: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BIO tag scheme\n",
    "all_tags = set(tag for tags in df[\"Tokens\"] for tag in tags)\n",
    "unique_tags = sorted(all_tags)\n",
    "label2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2label = {idx: tag for tag, idx in label2id.items()}\n",
    "\n",
    "# Add numeric label ids for each row\n",
    "df[\"Label_ids\"] = df[\"Tokens\"].apply(lambda tags: [label2id[tag] for tag in tags])\n",
    "\n",
    "# Train-validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "def to_hf_dataset(dataframe):\n",
    "    return Dataset.from_dict({\n",
    "        \"tokens\": dataframe[\"Tokenised Filled Template\"].tolist(),\n",
    "        \"labels\": dataframe[\"Label_ids\"].tolist()\n",
    "    })\n",
    "\n",
    "train_dataset = to_hf_dataset(train_df)\n",
    "val_dataset = to_hf_dataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "# model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "model_checkpoint = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",   # ensure uniform tensor sizes\n",
    "        max_length=128,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # For subword tokens, assign -100 or the same label as the word\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(predictions_output):\n",
    "    preds, labels = predictions_output\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        pred_tags = []\n",
    "        label_tags = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l != -100:\n",
    "                pred_tags.append(id2label[p])\n",
    "                label_tags.append(id2label[l])\n",
    "        true_preds.append(pred_tags)\n",
    "        true_labels.append(label_tags)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./final_ner_model\")\n",
    "tokenizer.save_pretrained(\"./final_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./final_ner_model\", tokenizer=\"./final_ner_model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Dr. Marvin Rolfson and Julius Daugherty attended the arbitration.\"\n",
    "print(ner_pipeline(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
